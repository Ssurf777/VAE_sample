{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11805PMHVLbuyjTjlOuU2IQCmgBuj10oB",
      "authorship_tag": "ABX9TyOkUQJCtTlegWPDkC51zhMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ssurf777/VAE_sample/blob/main/VAE_Chair_heat_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE"
      ],
      "metadata": {
        "id": "0KXmrO5hfjX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
        "from torchvision import transforms, utils\n",
        "import scipy.spatial.distance\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.interpolate import griddata\n",
        "import random\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "IsNpTKkl7Y7L"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ランダムシードの設定\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "pce5-43o-hQD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_off(file):\n",
        "    if 'OFF' != file.readline().strip():\n",
        "        raise ValueError('Not a valid OFF header')\n",
        "    n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
        "    verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
        "    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
        "    return verts, faces\n",
        "\n",
        "\n",
        "# 内挿域サンプリング\n",
        "class PointSampler(object):\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, int)\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def triangle_area(self, pt1, pt2, pt3):\n",
        "        side_a = np.linalg.norm(pt1 - pt2)\n",
        "        side_b = np.linalg.norm(pt2 - pt3)\n",
        "        side_c = np.linalg.norm(pt3 - pt1)\n",
        "        s = 0.5 * (side_a + side_b + side_c)\n",
        "        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n",
        "\n",
        "    def sample_point(self, pt1, pt2, pt3):\n",
        "        s, t = sorted([random.random(), random.random()])\n",
        "        f = lambda i: s * pt1[i] + (t - s) * pt2[i] + (1 - t) * pt3[i]\n",
        "        return (f(0), f(1), f(2))\n",
        "\n",
        "    def __call__(self, mesh):\n",
        "        verts, faces = mesh\n",
        "        verts = np.array(verts)\n",
        "        areas = np.zeros((len(faces)))\n",
        "\n",
        "        for i in range(len(areas)):\n",
        "            areas[i] = self.triangle_area(verts[faces[i][0]],\n",
        "                                          verts[faces[i][1]],\n",
        "                                          verts[faces[i][2]])\n",
        "\n",
        "        sampled_faces = random.choices(faces, weights=areas, k=self.output_size)\n",
        "        sampled_points = np.zeros((self.output_size, 3))\n",
        "\n",
        "        for i in range(len(sampled_faces)):\n",
        "            sampled_points[i] = self.sample_point(verts[sampled_faces[i][0]],\n",
        "                                                  verts[sampled_faces[i][1]],\n",
        "                                                  verts[sampled_faces[i][2]])\n",
        "        return sampled_points\n",
        "\n",
        "# クラスタリング\n",
        "def create_clusters(points, k=4):\n",
        "    distances = torch.cdist(points, points)\n",
        "    clusters = []\n",
        "    for i in range(len(points)):\n",
        "        closest_neighbors_idx = torch.topk(distances[i], k + 1, largest=False)[1][1:]\n",
        "        clusters.append(points[closest_neighbors_idx])\n",
        "    return clusters\n",
        "\n",
        "def create_clusters_with_kmeans(points, k=4, n_clusters=10):\n",
        "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42).fit(points.detach().cpu().numpy())  # detach() を追加\n",
        "    labels = kmeans.labels_\n",
        "    clusters = []\n",
        "\n",
        "    for label in range(n_clusters):\n",
        "        cluster_points = points[torch.tensor(labels) == label]\n",
        "        if len(cluster_points) > k:\n",
        "            clusters.extend(create_clusters(cluster_points, k=k))\n",
        "        else:\n",
        "            clusters.append(cluster_points)\n",
        "\n",
        "    return clusters\n",
        "\n",
        "\n",
        "# ラプラシアンの計算\n",
        "def compute_laplacian_with_clusters(model, clusters):\n",
        "    laplacians = []\n",
        "    for cluster in clusters:\n",
        "        cluster.requires_grad_(True)\n",
        "        T = model(cluster)\n",
        "\n",
        "        grad_T = torch.autograd.grad(outputs=T, inputs=cluster,\n",
        "                                     grad_outputs=torch.ones_like(T),\n",
        "                                     create_graph=True)[0]\n",
        "\n",
        "        laplacian = 0\n",
        "        for i in range(cluster.shape[1]):\n",
        "            grad2_T = torch.autograd.grad(grad_T[:, i], cluster,\n",
        "                                          grad_outputs=torch.ones_like(grad_T[:, i]),\n",
        "                                          create_graph=True, retain_graph=True)[0][:, i]\n",
        "            laplacian += grad2_T\n",
        "\n",
        "        laplacians.append(laplacian.mean())\n",
        "    return torch.stack(laplacians)\n",
        "\n",
        "# PINNの学習\n",
        "def train_pinn_with_clusters(model, points, heat_source, epochs=100, k=4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    points = points.to(device)\n",
        "    heat_source = heat_source.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    clusters = create_clusters_with_kmeans(points, k=k)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, point in enumerate(points):\n",
        "            distance_to_source = torch.norm(point - heat_source)\n",
        "            expected_temperature = 1 / (distance_to_source + 1e-8)\n",
        "\n",
        "            laplacian = compute_laplacian_with_clusters(model, [clusters[i]])\n",
        "            #laplacian = compute_laplacian_with_clusters_batch(model, [clusters[i]], batch_size=32)\n",
        "            loss_laplacian = torch.mean(laplacian[0] ** 2)\n",
        "            loss_temperature = torch.mean((model(point.unsqueeze(0)) - expected_temperature) ** 2)\n",
        "\n",
        "            loss = loss_laplacian + loss_temperature\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {total_loss / len(points)}')\n"
      ],
      "metadata": {
        "id": "FtV4wJWCd3AU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = [f\"/content/drive/MyDrive/chair/chair_{i:04d}.off\" for i in range(1,5)]\n",
        "\n",
        "input_data_list = []\n",
        "input_data_list2 = []\n",
        "\n",
        "for file_name in file_names:\n",
        "  with open(file_name, 'r') as f:\n",
        "    verts, faces = read_off(f)\n",
        "\n",
        "    i,j,k = np.array(faces).T\n",
        "    x,y,z = np.array(verts).T\n",
        "\n",
        "    pointcloud = PointSampler(500)((verts,faces))\n",
        "\n",
        "    # traindata\n",
        "    train_x = pointcloud[:,0]\n",
        "    train_y = pointcloud[:,1]\n",
        "    train_z = pointcloud[:,2]\n",
        "\n",
        "    train_xn = (train_x - train_x.min())/(train_x.max() - train_x.min())\n",
        "    train_yn = (train_y - train_y.min())/(train_y.max() - train_y.min())\n",
        "    train_zn = (train_z - train_z.min())/(train_z.max() - train_z.min())\n",
        "\n",
        "    # data combine\n",
        "    combined_data = np.concatenate((train_xn,train_yn,train_zn))\n",
        "\n",
        "    # input data\n",
        "    input_data_list.append(combined_data)\n",
        "    combined_data2 = np.vstack((train_xn, train_yn, train_zn)).T\n",
        "    input_data_list2.append(combined_data2)\n",
        "\n",
        "  input_data = np.stack(input_data_list)\n",
        "  #input_tensor = torch.tensor(input_data, dtype=torch.float32).cuda()\n",
        "  input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
        "  batch_size = 1\n",
        "  dataset = TensorDataset(input_tensor)\n",
        "  data_loader = DataLoader(dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "rQqVFEk-md1F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training setting\n",
        "eta = 0.001\n",
        "epochs = 500\n",
        "batch_size = 1\n",
        "interval = 10"
      ],
      "metadata": {
        "id": "Z5eOgzI5630U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_points = 500\n",
        "n_in_out = 3 * num_points\n",
        "n_make_out = 4 * num_points\n",
        "n_mid1 = 512\n",
        "n_mid2 = 256\n",
        "n_mid3 = 128\n",
        "n_mid4 = 64\n",
        "n_mid5 = 32\n",
        "n_z = 3 # 潜在変数の数\n",
        "\n",
        "class PointnetVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PointnetVAE, self).__init__()\n",
        "\n",
        "        # PointNet Encoder\n",
        "        self.conv1 = nn.Conv1d(n_in_out, 64, kernel_size=1)  # 通常は1D-CNNの後にbatchnormを入れるがbatchsize=1なので無\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=1)  # 通常は1D-CNNの後にbatchnormを入れるがbatchsize=1なので無\n",
        "        self.conv3 = nn.Conv1d(128, 1024, kernel_size=1) # 通常は1D-CNNの後にbatchnormを入れるがbatchsize=1なので無\n",
        "        # self.maxpool = nn.MaxPool1d(kernel_size=num_points)\n",
        "        self.maxpool = nn.AdaptiveMaxPool1d(1)  # 出力サイズを常に1に設定\n",
        "        self.linear1 = nn.Linear(1024, 512)\n",
        "        self.linear2 = nn.Linear(512, 256)\n",
        "        self.linear3 = nn.Linear(256, 9)\n",
        "        self.enc_mu = nn.Linear(9, n_z)\n",
        "        self.enc_logvar = nn.Linear(9, n_z)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec1 = nn.Linear(n_z, n_mid5)\n",
        "        self.dec2 = nn.Linear(n_mid5, n_mid4)\n",
        "        self.dec3 = nn.Linear(n_mid4, n_mid3)\n",
        "        self.dec4 = nn.Linear(n_mid3, n_mid2)\n",
        "        self.dec5 = nn.Linear(n_mid2, n_mid1)\n",
        "        self.dec_out = nn.Linear(n_mid1, n_make_out) #decoderからはx,y,z,Tが出力される\n",
        "\n",
        "        # weight initial\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu, logvar = self.encode(x)\n",
        "        y = self.decode(z)\n",
        "        return y, z, mu, logvar  # 4つの値を返す\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = x.reshape(1, n_in_out, 1)\n",
        "        x = F.leaky_relu(self.conv1(x), negative_slope=0.001)\n",
        "        x = F.leaky_relu(self.conv2(x), negative_slope=0.001)\n",
        "        x = F.leaky_relu(self.conv3(x), negative_slope=0.001)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(x.size(0), -1)  # フラット化してlinear層に接続\n",
        "        x = F.leaky_relu(self.linear1(x), negative_slope=0.001)\n",
        "        x = F.leaky_relu(self.linear2(x), negative_slope=0.001)\n",
        "        x = self.linear3(x)\n",
        "        mu = self.enc_mu(x)\n",
        "        logvar = self.enc_logvar(x)\n",
        "        std = torch.exp(0.5 * logvar)  # sigma\n",
        "        eps = torch.randn_like(std)  # 正規分布に従う乱数\n",
        "        z = mu + std * eps  # 潜在変数\n",
        "        self.mu = mu\n",
        "        self.logvar = logvar\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = F.leaky_relu(self.dec1(z), negative_slope=0.001)\n",
        "        x = F.leaky_relu(self.dec2(x), negative_slope=0.001)\n",
        "        x = F.leaky_relu(self.dec3(x), negative_slope=0.001)\n",
        "        x = F.leaky_relu(self.dec4(x), negative_slope=0.001)\n",
        "        x = F.leaky_relu(self.dec5(x), negative_slope=0.001)\n",
        "        x = torch.sigmoid(self.dec_out(x))\n",
        "        return x\n",
        "\n",
        "    def loss(self, recon_x, x, mu, logvar):\n",
        "        # デコーダからの出力を3次元の位置情報と温度に分ける\n",
        "        recon_x = recon_x.view(-1, num_points, 4)\n",
        "        recon_points = recon_x[:, :, :3]  # x, y, zを取得\n",
        "        recon_temperature = recon_x[:, :, 3]  # Tを取得\n",
        "\n",
        "        # オリジナルの入力も3次元の位置情報に整形\n",
        "        x = x.view(-1, num_points, 3)\n",
        "\n",
        "        # 再構成誤差 (MSE) の計算\n",
        "        rec_loss = F.mse_loss(recon_points, x, reduction=\"sum\")\n",
        "\n",
        "        # KLダイバージェンスによる正則化項の計算\n",
        "        reg_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        # クラスタの作成\n",
        "        clusters = create_clusters_with_kmeans(recon_points.view(-1, 3), k=4, n_clusters=10)\n",
        "\n",
        "        # ラプラシアンの計算\n",
        "        laplacian = compute_laplacian_with_clusters(self.decode, clusters)\n",
        "        loss_laplacian = torch.mean(laplacian ** 2)\n",
        "\n",
        "        # 温度損失の計算\n",
        "        heat_source = torch.tensor([0.0, 0.0, 0.0]).to(recon_points.device)  # 熱源の位置を定義\n",
        "        loss_temperature = 0\n",
        "        for i, point in enumerate(recon_points):\n",
        "            distance_to_source = torch.norm(point - heat_source, dim=1)\n",
        "            expected_temperature = 1 / (distance_to_source + 1e-8)\n",
        "            loss_temperature += torch.mean((recon_temperature[i] - expected_temperature) ** 2)\n",
        "        loss_temperature /= len(recon_points)\n",
        "        return rec_loss, reg_loss, loss_laplacian, loss_temperature\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "vae = PointnetVAE()\n",
        "print(vae)\n",
        "#vae.cuda()"
      ],
      "metadata": {
        "id": "T_I4tnow7M6r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be21803-9b0c-43f5-ae5b-e4a953ac464c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PointnetVAE(\n",
            "  (conv1): Conv1d(1500, 64, kernel_size=(1,), stride=(1,))\n",
            "  (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
            "  (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
            "  (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
            "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (linear3): Linear(in_features=256, out_features=9, bias=True)\n",
            "  (enc_mu): Linear(in_features=9, out_features=3, bias=True)\n",
            "  (enc_logvar): Linear(in_features=9, out_features=3, bias=True)\n",
            "  (dec1): Linear(in_features=3, out_features=32, bias=True)\n",
            "  (dec2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (dec3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (dec4): Linear(in_features=128, out_features=256, bias=True)\n",
            "  (dec5): Linear(in_features=256, out_features=512, bias=True)\n",
            "  (dec_out): Linear(in_features=512, out_features=2000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) # warningの非表示化\n",
        "\n",
        "optimizer = optim.Adam(vae.parameters(), lr=eta)\n",
        "\n",
        "rec_error_record = []\n",
        "reg_error_record = []\n",
        "laplacian_error_record = []\n",
        "temperature_error_record = []\n",
        "total_error_record = []\n",
        "z_list = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    vae.train()\n",
        "    loss_rec = 0\n",
        "    loss_reg = 0\n",
        "    loss_laplacian = 0\n",
        "    loss_temperature = 0\n",
        "    loss_total = 0\n",
        "\n",
        "    for j, (x,) in enumerate(data_loader):\n",
        "        #x = x.cuda()\n",
        "        y, z, mu, logvar = vae(x)  # muとlogvarを返すようにforward関数を修正する\n",
        "        #total_loss = vae.loss(y, x, mu, logvar)  # 修正後のloss関数を使用\n",
        "        lrec, lreg, llaplacian, ltemperature = vae.loss(y, x, mu, logvar)\n",
        "        # それぞれの損失項を合計して total_loss を計算する\n",
        "        total_loss = lrec + lreg + llaplacian + ltemperature\n",
        "        loss_rec += lrec.item()\n",
        "        loss_reg += lreg.item()\n",
        "        loss_laplacian += llaplacian.item()\n",
        "        loss_temperature += ltemperature.item()\n",
        "        loss_total += total_loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i == epochs - 1:\n",
        "            z_list.append(z.cpu().detach().numpy())\n",
        "\n",
        "    loss_rec /= j + 1\n",
        "    loss_reg /= j + 1\n",
        "    loss_laplacian /= j + 1\n",
        "    loss_temperature /= j + 1\n",
        "    loss_total /= j + 1\n",
        "\n",
        "    rec_error_record.append(loss_rec)\n",
        "    reg_error_record.append(loss_reg)\n",
        "    laplacian_error_record.append(loss_laplacian)\n",
        "    temperature_error_record.append(loss_temperature)\n",
        "    total_error_record.append(loss_total)\n",
        "\n",
        "    if i % interval == 0:\n",
        "        print(f\"Epoch:{i} Loss_Rec:{loss_rec} Loss_Reg:{loss_reg} Loss_Laplacian:{loss_laplacian} Loss_Temperature:{loss_temperature} Loss_Total:{loss_total}\")\n",
        "\n",
        "z_list = np.concatenate(z_list, axis=0)\n"
      ],
      "metadata": {
        "id": "m640f6fV8DfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "207572fe-20c4-45db-b1f9-f0487155ee9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 Loss_Rec:138.2219009399414 Loss_Reg:0.07221345603466034 Loss_Laplacian:6.65918577329172e-08 Loss_Temperature:0.4254498779773712 Loss_Total:138.71956253051758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(rec_error_record)+1),rec_error_record,label=\"MSE\")\n",
        "plt.plot(range(1, len(reg_error_record)+1),reg_error_record,label=\"D_KL\")\n",
        "plt.plot(range(1, len(laplacian_error_record)+1),laplacian_error_record,label=\"Laplacian\")\n",
        "plt.plot(range(1, len(temperature_error_record)+1),temperature_error_record,label=\"Temperature\")\n",
        "\n",
        "plt.plot(range(1, len(total_error_record)+1),total_error_record,label=\"Total\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vl1Nj1vj8KJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def plot_point_cloud(xs, ys, zs, title=\"Point Cloud\", ax=None):\n",
        "    if ax is None:\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        show_plot = True\n",
        "    else:\n",
        "        show_plot = False\n",
        "\n",
        "    ax.scatter(xs, ys, zs, s=1)\n",
        "    ax.set_title(title)\n",
        "    if show_plot:\n",
        "        plt.show()\n",
        "\n",
        "def plot_point_cloud_with_temperature(xs, ys, zs, temperatures, title=\"Point Cloud with Temperature\", ax=None):\n",
        "    if ax is None:\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        show_plot = True\n",
        "    else:\n",
        "        show_plot = False\n",
        "        # Get the figure from the axes object\n",
        "        fig = ax.get_figure()\n",
        "\n",
        "    # 温度に基づいてポイントクラウドの色を設定\n",
        "    scatter = ax.scatter(xs, ys, zs, c=temperatures, cmap='jet', s=20)\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # カラーバーを追加して温度のスケールを表示\n",
        "    fig.colorbar(scatter, ax=ax, label='Temperature')\n",
        "\n",
        "    if show_plot:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# 潜在変数z_listを2次元に縮退\n",
        "perplexity = 3  # perplexityを5に設定\n",
        "tsne = TSNE(n_components=2, perplexity=perplexity)\n",
        "z_list_2d = tsne.fit_transform(z_list)\n",
        "\n",
        "# z_list_2dの範囲を取得\n",
        "z1_min, z1_max = z_list_2d[:, 0].min(), z_list_2d[:, 0].max()\n",
        "z2_min, z2_max = z_list_2d[:, 1].min(), z_list_2d[:, 1].max()\n",
        "\n",
        "print(f\"z1 range: {z1_min} to {z1_max}\")\n",
        "print(f\"z2 range: {z2_min} to {z2_max}\")\n"
      ],
      "metadata": {
        "id": "tXRtMeVT8QUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ループして潜在変数を変化させ、再構築と可視化を行う\n",
        "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(20, 16), subplot_kw={'projection': '3d'})\n",
        "axes = axes.flatten()\n",
        "\n",
        "index = 0\n",
        "for z1 in np.arange(z1_min, z1_max + 0.2, 10):\n",
        "    for z2 in np.arange(z2_min, z2_max + 0.2, 10):\n",
        "        if index >= len(axes):\n",
        "            plt.show()\n",
        "            fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(20, 16), subplot_kw={'projection': '3d'})\n",
        "            axes = axes.flatten()\n",
        "            index = 0\n",
        "\n",
        "        # t-SNEの逆変換を行うために最近傍の16次元の潜在変数を取得\n",
        "        dist = np.linalg.norm(z_list_2d - np.array([z1, z2]), axis=1)\n",
        "        nearest_index = np.argmin(dist)\n",
        "        z_mod = torch.tensor(z_list[nearest_index], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # 再構築\n",
        "        recon_batch2 = vae.decode(z_mod)\n",
        "        reconst2 = recon_batch2.detach().numpy().reshape(-1)\n",
        "\n",
        "        reconst2_x = reconst2[:500]\n",
        "        reconst2_y = reconst2[500:1000]\n",
        "        reconst2_z = reconst2[1000:1500]\n",
        "        reconst2_T = reconst2[1500:2000]\n",
        "\n",
        "        title = f\"Point Cloud for z1={z1:.1f}, z2={z2:.1f}\"\n",
        "        plot_point_cloud_with_temperature(reconst2_x, reconst2_y, reconst2_z, reconst2_T, title, axes[index])\n",
        "        index += 1\n",
        "\n",
        "# 最後に残ったプロットを表示\n",
        "if index > 0:\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Gdez2pGY8RPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aRKY8yENJN_m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}